{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: grid; place-items: center; width: 100%;\">\n",
    "    <div style=\"font-size: 28px; font-weight: bold; text-align: center; margin-bottom: 20px;\"></div>\n",
    "    <div style=\"font-size: 28px; font-weight: bold; text-align: center; margin-bottom: 20px;\">Data Science in Particle Physics</div>\n",
    "    <div style=\"font-size: 22px; font-weight: bold; text-align: center; margin-bottom: 20px;\">Capstone Project 2023/2024</div>\n",
    "    <div style=\"font-size: 16px; text-align: center; margin-bottom: 20px;\">Ricardo Vicente</div>\n",
    "    <img src=\"higgsboson/higgs_boson.jpg\" style=\"max-width: 70%; display: block; margin: 0 auto;\">\n",
    "    <div style=\"font-size: 28px; font-weight: bold; text-align: center; margin-bottom: 20px;\"></div>\n",
    "</div>\n",
    "\n",
    "The Higgs boson is an elementary particle in the Standard Model of particle physics. It is the *quantum* excitation of the Higgs field, which permeates all of space. The Higgs field is responsible for giving mass to other elementary particles through the mechanism of spontaneous symmetry breaking. Without the Higgs field, particles like quarks and electrons would be massless, and the universe would be vastly different. At high energies, the Higgs field has a symmetrical state with zero value. As the universe cooled, the field took on a non-zero value in all of space, breaking the symmetry. Particles interacting with the Higgs field gain mass. The more strongly a particle interacts with the Higgs field, the more massive it becomes.\n",
    "\n",
    "The Higgs boson was first proposed by Peter Higgs in 1964, along with independent contributions by François Englert and Robert Brout, as well as other physicists. The prediction emerged from the need to explain how particles acquire mass. The electroweak theory, which unifies the electromagnetic force and weak nuclear force, required a mechanism to endow particles with mass while preserving the gauge symmetry of the theory. The Higgs mechanism provided this solution.\n",
    "\n",
    "The confirmation of the Higgs boson took place at CERN (the European Organization for Nuclear Research) in Switzerland, using the Large Hadron Collider (LHC). The LHC is the world’s largest and most powerful particle collider, capable of accelerating protons to near-light speeds and then smashing them together. Two main experiments at the LHC, ATLAS and CMS, were designed to detect the Higgs boson among other particles. These detectors are vast, sophisticated machines capable of tracking the particles produced by collisions. Protons were collided at very high energies, producing a wide range of particles. The Higgs boson is unstable and decays into other particles almost immediately. Physicists looked for specific patterns in the decay products that would indicate the presence of a Higgs boson. Some key decay channels included: two photons (γγ), two Z bosons (ZZ) which further decay into four leptons, two W bosons (WW), two tau leptons (ττ). Enormous amounts of data were analyzed to identify excess events over the expected background that matched the predicted properties of the Higgs boson. On July 4, 2012, CERN announced that both the ATLAS and CMS experiments had observed a new particle with a mass around 125-126 GeV (giga-electronvolts), consistent with the predicted Higgs boson. \n",
    "\n",
    "Data Science played a crucial role in the discovery of the Higgs boson. The process involved massive amounts of data collection, processing, and analysis, which required advanced Data Science techniques.\n",
    "\n",
    "1. **Data Collection**\n",
    "ATLAS and CMS Detectors collected data from billions of proton-proton collisions per second. Each collision produced a vast amount of data, leading to petabytes of data being generated annually.\n",
    "    \n",
    "3. **Data Processing**\n",
    "Real-time data filtering systems called triggers were used to identify potentially interesting events. These systems reduced the data from billions of events per second to a manageable number of events for further analysis. Sophisticated algorithms selected events that matched the expected signature of Higgs boson decays. The Worldwide LHC Computing Grid (WLCG) was used to store and process the massive amounts of data. It linked computing resources from institutions around the world.\n",
    "4. **Data Analysis**\n",
    "Scientists used hypothesis testing to determine if the observed data deviated significantly from the background noise and matched the expected signal for the Higgs boson. P-values were calculated to determine the statistical significance of the observed excess. A discovery required a 5-sigma significance level, meaning the probability of the result being due to random chance was less than one in 3.5 million. Machine learning algorithms were employed to recognize complex patterns in the data and to distinguish between signal (potential Higgs boson events) and background (other types of events); supervised learning techniques were used to classify events based on their characteristics, improving the accuracy of identifying Higgs boson events.\n",
    "\n",
    "## Phase 1: Frame the Problem \n",
    "\n",
    "Collisions in high-energy particle accelerators are crucial for discovering exotic particles and require Data Analysis approaches to solve 'signal' (1) versus 'background' (0) classification problems. **'Shallow' Machine Learning models** have been limited in their ability to learn complex nonlinear functions, leading to a meticulous search for **manually constructed nonlinear features**.\n",
    "\n",
    "Recent advances in the field of **Deep Learning** have allowed for learning more complex functions **directly from the data**, without the need for manually constructed features. Using benchmark datasets, Deep Learning methods have been shown to significantly improve classification metrics by up to 8% over the best current approaches.\n",
    "\n",
    "While current approaches in high-energy physics have improved with manually constructed physics-inspired features, Deep Learning has proven to be more effective in automatically discovering powerful combinations of nonlinear features. Advances in Deep Learning, such as the use of neural networks with multiple hidden layers, have shown promise in improving the discrimination capability between signals and backgrounds in high-energy physics.\n",
    "\n",
    "These developments in Deep Learning could inspire new advances in Data Analysis tools to address more complex problems in high-energy physics and could have applications in identifying decay products in high-dimensional raw detector data.\n",
    "\n",
    "\n",
    "### 1.\tPresentation of the chosen dataset\n",
    "\n",
    "[The Higgs boson detection data](https://archive.ics.uci.edu/dataset/280/higgs) has been produced using Monte Carlo simulations. The first 17 features (columns 1-17) represent kinematic properties measured by particle detectors in the accelerator (Low-Level). The last 7 features (columns 18-24) are functions of the first 17, designed by physicists to differentiate between the two classes (High-Level).\n",
    "\n",
    "The variable set includes Low-Level features, which are direct measurements from particle detectors, and High-Level features, which are combinations of the former. These variables describe various aspects of particle collisions, ranging from the energy and momentum of individual particles to combined properties of multiple particles.\n",
    "\n",
    "\n",
    "### 2.\tGeneral Features\n",
    "\n",
    "This dataset belongs to the \"Classification in Numerical Features\" benchmark. It is a [smaller version](https://www.openml.org/search?type=data&status=active&id=45570&sort=runs) (about 2.14%) of the original dataset, which contains 44 million rows. This version corrects the previous one in the definition of the class attribute, which is categorical, not numerical. \n",
    "\n",
    "The author of this version is Daniel Whiteson from the University of California, Irvine, who selected 50% of experiments with ‘background’ and 50% with ‘signal’; it is, therefore, a balanced dataset.\n",
    " \n",
    "\n",
    "### 3.\tDefinition of the variables\n",
    "\n",
    "The variables are divided into 3 groups: kinematic or Low-Level, derived or High-Level, and the class label:\n",
    "\n",
    "<i>**i. Kinematic or Low-Level features (numeric):**</i>\n",
    "<ol>\n",
    "    <li><strong>lepton_pT</strong> (Lepton transverse momentum)</li>\n",
    "    <li><strong>lepton_eta</strong> (Lepton pseudorapidity)</li>\n",
    "    <li><strong>lepton_phi</strong> (Lepton azimuthal angle)</li>\n",
    "    <li><strong>missing_energy_magnitude</strong> (Missing energy magnitude)</li>\n",
    "    <li><strong>missing_energy_phi</strong> (Missing energy azimuthal angle)</li>\n",
    "    <li><strong>jet_1_pt</strong> (First jet transverse momentum)</li>\n",
    "    <li><strong>jet_1_eta</strong> (First jet pseudorapidity)</li>\n",
    "    <li><strong>jet_1_phi</strong> (First jet azimuthal angle)</li>\n",
    "    <li><strong>jet_2_pt</strong> (Second jet transverse momentum)</li>\n",
    "    <li><strong>jet_2_eta</strong> (Second jet pseudorapidity)</li>\n",
    "    <li><strong>jet_2_phi</strong> (Second jet azimuthal angle)</li>\n",
    "    <li><strong>jet_3_pt</strong> (Third jet transverse momentum)</li>\n",
    "    <li><strong>jet_3_eta</strong> (Third jet pseudorapidity)</li>\n",
    "    <li><strong>jet_3_phi</strong> (Third jet azimuthal angle)</li>\n",
    "    <li><strong>jet_4_pt</strong> (Fourth jet transverse momentum)</li>\n",
    "    <li><strong>jet_4_eta</strong> (Fourth jet pseudorapidity)</li>\n",
    "    <li><strong>jet_4_phi</strong> (Fourth jet azimuthal angle)</li>\n",
    "</ol>\n",
    "\n",
    "<i>**ii. Derived or High-Level features engineered by physicists (numeric):**</i>\n",
    "<ol start=\"18\">\n",
    "    <li><strong>m_jj</strong> (Invariant mass of the dijet system)</li>\n",
    "    <li><strong>m_jjj</strong> (Invariant mass of the trijet system)</li>\n",
    "    <li><strong>m_lv</strong> (Invariant mass of the lepton-neutrino system)</li>\n",
    "    <li><strong>m_jlv</strong> (Invariant mass of the jet-lepton-neutrino system)</li>\n",
    "    <li><strong>m_bb</strong> (Invariant mass of the two b-tagged jets system)</li>\n",
    "    <li><strong>m_wbb</strong> (Invariant mass of the jet-lepton-neutrino and two b-tagged jets system)</li>\n",
    "    <li><strong>m_wwbb</strong> (Invariant mass of the jet-lepton-neutrino and four b-tagged jets system)</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "<i>**iii. Class label (categorical):**</i>\n",
    "<ol start=\"25\">\n",
    "    <li><strong>label</strong> (Used to distinguish between signal [1] and background [0].)</li>\n",
    "</ol>\n",
    "\n",
    "### 4.\tPresentation of the objectives\n",
    "\n",
    "This is a classification problem to distinguish between a **signal** process produced by Higgs bosons and a **background** process that does not.\n",
    "\n",
    "1. The first objective of this work is to analyze the **behavior of two types of models**: one more **'superficial'**, such as the Extreme Gradient Boosting Classifier, and others of **Deep Learning**, such as the Multilayer Perceptrons, using the libraries Keras/Tensorflow and Scikit-learn.\n",
    "    \n",
    "2. The second objective concerns the **variables**. Which model better adapts to the Low/High-Level variables? With which types of variables do the models make better predictions? Will the models be capable of learning complex patterns in **kinematic data** to accurately classify observations into different categories (signal or background), without requiring manual intervention by physicists to create **additional features**?\n",
    " \n",
    "### 5.\tProject phases\n",
    "<ol>\n",
    "    <li>Frame the Problem</li>\n",
    "    <li>Get the Data</li>\n",
    "    <li>Visualize the Data</li>\n",
    "    <li>Prepare the Data</li>\n",
    "    <li>Select, Train and Fine-Tune the Models</li>\n",
    "    <li>Conclusions</li>\n",
    "</ol>\n",
    "\n",
    "### 6.\tReference\n",
    "\n",
    "Baldi, P. et al. <i>Searching for exotic particles in high-energy physics with deep learning.</i> Nat. Commun. 5:4308 doi: 10.1038/ncomms5308 (2014).\n",
    "\n",
    "Baldi, P. et al. <i>Enhanced Higgs to τ +τ − Search with Deep Learning.</i> Physical Review Letters 114(11) doi: 10.1103/PhysRevLett.114.111801 (2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Get the Data\n",
    "\n",
    "\n",
    "The \"Get the Data\" phase in a Python data science project marks the crucial initial step of acquiring the dataset necessary for analysis and modeling. This phase involves setting up the project environment, including installing essential Python libraries and tools, and obtaining the dataset itself.\n",
    "\n",
    "In this phase, we'll typically:\n",
    "\n",
    "1. **Setup Workspace**: Create a dedicated directory or workspace for the project. This ensures organization and separation from other projects.\n",
    "    \n",
    "3. **Install Dependencies**: Install required Python libraries such as Jupyter, NumPy, Pandas, Matplotlib, Keras, Tensorflow, and Scikit-Learn. These libraries form the backbone of data manipulation, visualization, and Machine/ Deep Learning tasks.\n",
    "4. **Download Dataset**: Obtain the dataset needed for analysis. In the case of this project, this involves downloading a single file (dataset_higgs.csv).\n",
    "5. **First visualizations**: Plotting histograms for numerical attributes provides a quick overview of the dataset's distribution patterns. \n",
    "\n",
    "By completing the \"Get the Data\" phase effectively, we establish a solid foundation for our data science project. We ensure that we have access to the necessary data and the tools required for analysis and modeling. This sets the stage for subsequent phases such as data exploration, preprocessing, model building, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "This code loads and inspects a dataset from a CSV file using the pandas library. It specifies the file path to dataset_higgs.csv, reads the file into a DataFrame, and uses the first row as column names. The code then displays the first rows of the dataset to provide an initial view of the data's structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and print the first line\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# File path of the dataset\n",
    "file_path = \"./higgsboson/dataset_higgs.csv\"\n",
    "\n",
    "# Read the CSV file and use the first row as column names\n",
    "df = pd.read_csv(file_path, header=0)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line df.info() in the code is used to display a concise summary of the DataFrame df. This method provides essential information about the DataFrame, including: Index Range, Column Names and Types, and Non-Null Counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display quick description of the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset, as summarized by df.info(), consists of 940,160 rows and 25 columns. All columns have complete data without any missing values. Most columns store numerical data in floating-point format, representing measurements like particle transverse momentum (lepton_pT), energy (missing_energy_magnitude), and angles (lepton_phi, jet_1_phi, etc.). The label column is integer-type, likely indicating the classification of each entry, such as distinguishing between different types of particle interactions. Overall, this dataset appears well-suited for machine learning tasks, particularly classification based on its comprehensive and numerical feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following df.describe() function in pandas provides a statistical summary of the numerical attributes (columns) and label in the DataFrame df. It includes key statistical measures such as count, mean, standard deviation (std), minimum (min), quartiles (25th, 50th, and 75th percentiles), and maximum (max) values for each column. This summary helps analysts quickly grasp the central tendency, spread, and distribution of the data, identify potential outliers, and make informed decisions about data preprocessing or modeling strategies for tasks such as machine learning, deep learning and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display summary of the attributes \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the dataset summary provided, here are the overall tendencies observed across the attributes:\n",
    "\n",
    "1. **Symmetry in Distributions**: Attributes such as lepton_eta, lepton_phi, and missing_energy_phi exhibit distributions symmetrically centered around zero. This symmetry indicates that measurements related to angles and directions (eta and phi) are evenly distributed across positive and negative values.\n",
    "   \n",
    "3. **Variable Ranges**: Most attributes, including lepton_pT, missing_energy_magnitude, and various jet attributes (jet_1_pt to jet_4_phi), cover a wide range of values. This variability suggests significant diversity in the physical quantities measured, reflecting the complexity of particle interactions.\n",
    "4. **Moderate to High Variability**: Attributes such as lepton_pT, missing_energy_magnitude, and m_jj, m_jjj, m_lv, m_jlv, m_bb, m_wbb, m_wwbb (mass measurements) display moderate to high variability as indicated by their standard deviations. This variability implies that these attributes can significantly differ from one observation to another.\n",
    "5. **Balanced Dataset**: The label attribute, which likely represents a binary classification (possibly signal vs. background events), has a mean around 0.5. This suggests a balanced distribution between the two classes, which is important for training classifiers without a class imbalance bias.\n",
    "6. **Typical Values and Central Tendencies**: Attributes like lepton_pT and missing_energy_magnitude have means close to 1, indicating that their typical values are centered around this average. This central tendency helps in understanding the baseline values for these measurements.\n",
    "7. **Standard Deviations**: The standard deviations across various attributes provide insights into the spread or dispersion of data points around their respective means. Higher standard deviations indicate greater variability, while lower ones suggest more clustered data points.\n",
    "8. **Measurement Specificity**: Attributes related to jets (jet_1_pt to jet_4_phi) show specific measurements related to momentum (pt), pseudorapidity (eta), and azimuthal angle (phi) for up to four jets per event. Each of these attributes contributes uniquely to understanding the characteristics of jet formations in particle collisions.\n",
    "These overall tendencies provide a foundational understanding of the dataset's composition and variability, which is crucial for subsequent data exploration, feature engineering, and model development tasks in particle physics analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. First Visualizations\n",
    "This code snippet is used to visually explore and understand the distribution of the data attributes and label in the dataset. It helps in identifying the spread of values, detecting any skewness or outliers, gaining initial insights into the data characteristics, in conclusion, confirming the initial conclusions drawn earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables inline plotting in Jupyter notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Importing necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting histograms for each column in the original DataFrame\n",
    "df.hist(bins=50, figsize=(20, 15))\n",
    "plt.suptitle('First Visualizations of All Features', fontsize=23)\n",
    "plt.subplots_adjust(top=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Visualize the Data\n",
    "\n",
    "\n",
    "Now, let's dive deeper into the data to unearth valuable insights, namely:\n",
    "\n",
    "1. **Identifying Correlations utilizing Pearson's correlation coefficient (r)**. A positive r value close to 1 indicates a strong positive correlation, such as higher values of one feature correlating with higher values of another feature. Conversely, a negative r value near -1 indicates a strong negative correlation, where higher values of one feature are associated with lower values of another feature. Coefficients close to zero suggest no linear correlation between the features being analyzed.\n",
    "   \n",
    "3. **Employ Pandas' scatter_matrix function** for a visual representation of correlations between numerical attributes. The main diagonal presents histograms for individual attributes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code first shuffles the entire dataset df to randomize the order of its rows. Then, it selects a random sample containing 5% of the rows from the shuffled dataset (shuffled_df). Finally, it prints the sizes of both the shuffled dataset (shuffled_df) and the sampled subset (shuffled_sampled_df) to confirm the number of rows in each after these operations.\n",
    "    \n",
    "Shuffling data involves randomly reordering the dataset, which helps remove any inherent sequence biases. Sampling, on the other hand, involves selecting a subset of the shuffled data for visualization purposes or specific analysis tasks. These steps are crucial for ensuring data integrity, reducing biases, and facilitating effective data exploration through visual representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the train_set\n",
    "shuffled_df = df.sample(frac=1)  \n",
    "\n",
    "# Take a sample of 10% of the shuffled train_set\n",
    "shuffled_sampled_df = shuffled_df.sample(frac=0.05)\n",
    "\n",
    "print('Size of the train_set:', len(shuffled_df))\n",
    "print('Size of the shuffled sample:', len(shuffled_sampled_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Plot the Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CorrelationMatrix function calculates and visualizes the correlation matrix for specified columns in a Pandas DataFrame. It imports seaborn and matplotlib.pyplot for plotting. The function computes the correlation matrix for given columns (COLS) and the 'label' column from DATAFRAME. It then plots the matrix using sns.heatmap to show pairwise correlations. The plot uses colors to represent correlation strength (annot=True shows values), with adjustments for clarity (figsize=(45, 45)). It identifies top 5 variables correlated with 'label' and prints their values. This helps in exploring relationships and selecting features for analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CorrelationMatrix(DATAFRAME, COLS, columns):\n",
    "    \"\"\"\n",
    "    Plotting the correlation matrix for the specified columns in the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        DATAFRAME (DataFrame): Pandas DataFrame containing the data.\n",
    "        COLS (list): List of columns for which correlation matrix is calculated.\n",
    "        columns (str): String to describe the columns being analyzed.\n",
    "        \n",
    "    Returns:\n",
    "        top_variables (list): List of top 5 variables with the highest correlation to 'label'.\n",
    "    \"\"\"    \n",
    "    # Importing necessary libraries\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = DATAFRAME[COLS + ['label']].corr()\n",
    "\n",
    "    # Plot the correlation matrix\n",
    "    plt.figure(figsize=(45, 45))\n",
    "    heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".4f\", annot_kws={\"size\": 25}, xticklabels=True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=45)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "    plt.title(f\"Correlation Matrix for {columns}\", fontsize=60)\n",
    "    plt.show()\n",
    "    \n",
    "    # Find correlations of the 'label' with other variables\n",
    "    label_correlations = correlation_matrix['label'].abs().sort_values(ascending=False)\n",
    "    label_correlations = label_correlations[label_correlations.index != 'label']  # Exclude self-correlation\n",
    "\n",
    "    # Print the labels of the top 5 variables with the highest correlation to the label\n",
    "    print(f\"\\nTop 5 variables with the highest relation to 'label':\\n\")\n",
    "    for index in label_correlations.head(5).index:\n",
    "        correlation = label_correlations[index]\n",
    "        print(f\"\\t{index}: {correlation:.4f}\")\n",
    "    \n",
    "    top_variables = label_correlations.head(5).index.tolist()\n",
    "    \n",
    "    return top_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the following lists of column names (LowLevelCols and HighLevelCols) to categorize or group specific types of features or variables from our dataset. This organization can serve several purposes in data analysis tasks, such as 'Feature Selection'. LowLevelCols represents raw measurements or basic attributes, while HighLevelCols represents derived features or higher-level summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of variable sets\n",
    "LowLevelCols = ['lepton_pT', 'lepton_eta', 'lepton_phi', 'missing_energy_magnitude', 'missing_energy_phi', \\\n",
    "          'jet_1_pt', 'jet_1_eta', 'jet_1_phi', 'jet_2_pt', 'jet_2_eta', 'jet_2_phi', 'jet_3_pt', 'jet_3_eta', \\\n",
    "          'jet_3_phi', 'jet_4_pt', 'jet_4_eta', 'jet_4_phi']\n",
    "\n",
    "HighLevelCols = ['m_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line analyzes correlations among Low-Level features in shuffled_sampled_df, displaying them as a heatmap and identifying the top variables correlated with the 'label':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix for Low Level variables\n",
    "top_low_variables = CorrelationMatrix(shuffled_sampled_df, LowLevelCols, 'Low-level Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While these correlations suggest some level of relationship between each Low-Level variable and the label, they are not considered high. Typically, correlation coefficients around 0.1 to 0.3 are considered moderate, while those below 0.1 are considered weak. Therefore, in the context of correlation strength, these coefficients can be categorized as moderate to weak, with missing energy magnitude showing the strongest correlation among them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following line analyzes correlations among High-Level features in shuffled_sampled_df, displaying them as a heatmap and identifying the top variables correlated with the 'label'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix for High Level variables\n",
    "top_high_variables = CorrelationMatrix(shuffled_sampled_df, HighLevelCols, 'High-level Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this matrix, it can be observed that m_bb and m_wwbb show moderate correlations with the label, indicating they might exert a more significant influence compared to m_wbb, m_jlv, and m_jjj, which display weaker correlations. These relationships are crucial for understanding how variations in these variables could affect the label in your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These findings highlight that High-Level variables such as m_bb and m_wwbb are more strongly correlated with the Label compared to Low-Level variables. This suggests that changes in High-Level variables may have a more pronounced impact on the Label in your dataset analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Boxplot the Top Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BoxPlot function generates boxplots for columns in a DataFrame based on a binary classification label. It imports matplotlib.pyplot for plotting, calculates subplot layout based on COLS, and creates subplots. For each column, it extracts data for 'label' 0 and 1, then plots boxplots with labels '0' and '1'. Titles and axis labels are set, a main title for the figure is added, and the plot is displayed using plt.show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoxPlot(DATAFRAME, COLS, string):\n",
    "    \"\"\"\n",
    "    Generate boxplots for specified columns in a dataframe, labeling them based on a binary classification.\n",
    "\n",
    "    Parameters:\n",
    "        DATAFRAME (DataFrame): The pandas DataFrame containing the data.\n",
    "        COLS (list): List of column names for which boxplots are to be generated.\n",
    "        string (str): String describing the columns being analyzed.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Import libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Calculate number of rows and columns needed for subplots\n",
    "    num_cols = len(COLS)\n",
    "    num_rows = 1\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, 5))\n",
    "    \n",
    "    # Flatten axs if there is only one row\n",
    "    axs = axs.flatten() if num_rows == 1 else axs\n",
    "    \n",
    "    # Iterate over columns\n",
    "    for i, col in enumerate(COLS):\n",
    "        # Extract data for label 0 and label 1\n",
    "        label0_data = DATAFRAME.loc[DATAFRAME['label'] == 0, col]\n",
    "        label1_data = DATAFRAME.loc[DATAFRAME['label'] == 1, col]\n",
    "        \n",
    "        # Create boxplot for each column\n",
    "        axs[i].boxplot([label0_data, label1_data], positions=[0, 1], widths=0.6, tick_labels=['0', '1'])\n",
    "        axs[i].set_title(col)  # Set subplot title\n",
    "        \n",
    "        # Set labels and title\n",
    "        axs[i].set_xlabel('label')\n",
    "        axs[i].set_ylabel(col)\n",
    "        axs[i].set_xticks([0, 1])\n",
    "    \n",
    "    # Add main title\n",
    "    plt.suptitle(f\"Boxplots for top {string}\", fontsize=20)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following boxplots for the top features related to the label, the significance of outliers and differences between the boxes can provide critical insights into the data distribution and group distinctions:\n",
    "\n",
    "1. **Outliers**: Outliers in boxplots are individual data points that are significantly different from the rest of the data. They can indicate variability or errors in measurement but can also represent important anomalies or extremes in your dataset. Understanding outliers helps in assessing the overall distribution and potential impacts on statistical analyses.\n",
    "\n",
    "2. **Differences Between Boxes**: The differences between the boxes (especially their median, interquartile range, and whiskers) highlight variations between different categories or groups in your data. In the context of binary classification (labels '0' and '1'), these differences can signify how distinct the distributions of the data are between the two groups. Significant differences suggest that the variable (represented by the boxplot) may have predictive or discriminatory power in relation to the classification label.\n",
    "\n",
    "3. **Overall Importance**: Boxplots provide a visual summary of the distributional characteristics of your data, allowing us to quickly compare different variables or groups. They help identify outliers, visualize central tendency, spread, skewness, and potential differences between groups. Understanding these aspects is crucial for making informed decisions in statistical modeling, hypothesis testing, or feature selection tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot for the 5 top Low Level variables\n",
    "BoxPlot(shuffled_sampled_df, top_low_variables, 'Low-level Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot for the 5 top High Level variables\n",
    "BoxPlot(shuffled_sampled_df, top_high_variables, 'High-level Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plots underscore the superior correlation between the High-Level variables and the Label, through the significant differences in box distributions and medians between the two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Prepare the Data\n",
    "\n",
    "\n",
    "Before applying Machine Learning algorithms, it's necessary to prepare the data. This process involves several essential steps, like:\n",
    "\n",
    "1. **Handling Missing Values (NaN)**: Prior to feeding the data into Machine/ Deep Learning algorithms, it's crucial to address missing values (NaNs). This can be done by either removing the rows or columns containing NaNs, filling them with a specific value (like the mean or median of the column), or using more advanced techniques such as imputation.\n",
    "   \n",
    "3. **Normalization**: Normalizing the data ensures that all features have the same scale. This step is crucial for many algorithms, specially in Deep Learning. Normalization typically involves scaling numerical features to a standard range, or using standardization, which centers the feature columns at mean 0 with standard deviation 1.\n",
    "\n",
    "By preparing the data properly, we ensure it's clean, consistent, and appropriately scaled for effective model training and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\tHandling Missing Values (NaNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code checks for NaN values in each column of the DataFrame df and print whether each column contains NaNs. This helps identify which columns may have missing data that need to be handled in data preprocessing or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if the set has NaN values\n",
    "print(\"Do the columns in the dataset have NaN values?\")\n",
    "print(df.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All columns in the dataset do not have NaN values. Each column shows a \"False\" result, indicating no NaNs are present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalization\n",
    "The following code prepares df for Machine/ Deep Learning analysis by applying standardization to the numerical features, ultimately optimizing the data for predictive modeling tasks. The significance of this process lies in its ability to enhance model performance and ensure consistent preprocessing of numerical features across different datasets:\n",
    "\n",
    "1. **Enhanced Model Performance**: Standardization improves model performance by ensuring that features are on a comparable scale, which aids in better convergence and interpretation of model parameters.\n",
    "\n",
    "2. **Preprocessing Consistency**: By standardizing features, the code facilitates consistent handling of numerical data across different features, enhancing the reliability and interpretability of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling: Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the selected columns\n",
    "scaled_data = scaler.fit_transform(df[LowLevelCols + HighLevelCols])\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "df_std = pd.DataFrame(scaled_data, columns=LowLevelCols + HighLevelCols)\n",
    "\n",
    "# Add the 'label' column back to the scaled DataFrame\n",
    "df_std['label'] = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous output and the following graphics show that all columns are already normalized, with mean centered around zero and standard deviation of one, ensuring consistent scaling across features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables inline plotting in Jupyter notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Importing necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting histograms for each column in the standardized DataFrame\n",
    "df_std.hist(bins=50, figsize=(20, 15))\n",
    "plt.suptitle('Visualizations of All Features Standardized', fontsize=23)\n",
    "plt.subplots_adjust(top=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Select and Train the Models\n",
    "\n",
    "This stage involves choosing suitable Machine/ Deep Learning algorithms to solve the problem at hand and then training it on the available data. Once the preparatory steps are complete (preprocess the data, split it into training and test sets, and perform feature engineering), we select the models that we believe might perform well on the task based on our understanding of the problem and the characteristics of the data. Then, we train the selected models on the training set, optimizing its parameters to minimize error and/or maximize performance metrics. \n",
    "\n",
    "We could use here Grid-Search. It is a method used for fine-tuning the hyperparameters of a Machine/ Deep Learning model. Hyperparameters are parameters that are not directly learned from the data during training but instead control the learning process. Grid search involves defining a grid of hyperparameter values to explore, and then systematically training and evaluating the model for each combination of hyperparameters. This exhaustive search allows us to identify the combination of hyperparameters that results in the best performance on a validation set. Grid Search is a systematic way to find optimal hyperparameters and improve the model's performance; however, as it is computationally expensive, especially with a considerably large dataset, and we do not have the technical conditions to do so, it will not be carried out in this work. Instead, the tuning of the models will be done manually, through experimentation, that is, trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create the Training and the Test Sets\n",
    "\n",
    "The process of creating the Training and the Test sets involves random or stratified sampling to ensure that both sets represent the overall distribution of the data. Techniques like stratified sampling, as seen in the following code, are used to maintain the proportion of classes (if dealing with classification tasks) in both the training and test sets. This ensures that the model's evaluation on the test set reflects its true performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary library\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Initialize StratifiedShuffleSplit for stratified sampling\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "# Splitting data into training and test sets based on 'label' to maintain class proportionality\n",
    "for train_index, test_index in split.split(df_std, df_std['label']):\n",
    "    train_set_orig = df_std.loc[train_index]\n",
    "    test_set = df_std.loc[test_index]\n",
    "\n",
    "# Printing sizes of the training and test sets\n",
    "print(\"train_set_orig size:\", len(train_set_orig))\n",
    "print(\"test_set size:\", len(test_set))\n",
    "\n",
    "# Displaying label category proportions in the training and test sets\n",
    "print('\\nLabel category proportions in the train_set_orig:')\n",
    "print(train_set_orig['label'].value_counts() * 100 / len(train_set_orig))\n",
    "print('\\nLabel category proportions in the test_set:')\n",
    "print(test_set['label'].value_counts() * 100 / len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label category proportions in both sets indicate the percentage distribution of each class label (0 and 1) in their respective datasets. Both the training and test sets exhibit an equal distribution of labels, with each label comprising 50% of the dataset, mirroring the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation set plays a crucial role in ensuring that Machine/ Deep Learning models generalize well to new data and in optimizing their performance before final evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train_set_orig into training and validation sets\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42) \n",
    "\n",
    "# Reset index before splitting to avoid issues with indexing\n",
    "train_set_orig.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Perform the stratified split\n",
    "for train_index, val_index in split.split(train_set_orig, train_set_orig['label']):\n",
    "    train_set = train_set_orig.loc[train_index]\n",
    "    val_set = train_set_orig.loc[val_index]\n",
    "\n",
    "# Display sizes of the resulting sets\n",
    "print(\"train_set size:\", len(train_set))\n",
    "print(\"val_set size:\", len(val_set))\n",
    "\n",
    "# Display label category proportions in the train_set and val_set\n",
    "print('\\nLabel category proportions in the train_set:')\n",
    "print(train_set['label'].value_counts() * 100 / len(train_set))\n",
    "print('\\nLabel category proportions in the val_set:')\n",
    "print(val_set['label'].value_counts() * 100 / len(val_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation set also shows an equal distribution of both classes (0 and 1), each making up 50% of the validation set. This ensures that both sets maintain the same class distribution as the original dataset, facilitating unbiased model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Drop the Label\n",
    "Dropping the label refers to removing or excluding the column that contains the target variable (or label) from the dataset. This is typically done when preparing the data for machine learning tasks, where the goal is to separate the features (independent variables) from the target variable (dependent variable or label). By dropping the label, you ensure that the model does not inadvertently learn from the target variable during training, which could lead to biased results or overfitting. The label is usually dropped from the input features and stored separately as the target variable that the model aims to predict based on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the predictors and the labels in the training set\n",
    "X_train = train_set.drop(\"label\", axis=1)\n",
    "y_train = train_set[\"label\"].copy()\n",
    "\n",
    "# Separate the predictors and the labels in the validation set\n",
    "X_val = val_set.drop(\"label\", axis=1)\n",
    "y_val = val_set[\"label\"].copy()\n",
    "\n",
    "# Separate the predictors and the labels in the test set\n",
    "X_test = test_set.drop(\"label\", axis=1)\n",
    "y_test = test_set[\"label\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement recurrent functions \n",
    "At this moment, we are going to implement three functions: One function will compute **Statistics** such as accuracy, precision, and recall, which evaluate the performance and correctness of a classifier model. The second function will plot the **Receiver Operating Characteristic** (ROC) curve and calculate the Area Under the Curve (AUC), which assesses the model's ability to distinguish between classes based on predicted probabilities. The last function will **plot results** (Accuracy, Precision, and Recall) using bar plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Statistics for binary classification\n",
    "Accuracy, precision, and recall are fundamental metrics used to evaluate the performance of classification models. Each metric provides unique insights into different aspects of the model's predictive ability, especially in scenarios where the distribution of classes in the dataset varies:\n",
    "\n",
    "1. **Accuracy** measures the overall correctness of the model but can be misleading for imbalanced datasets:\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Number of Instances}}\n",
    "$$\n",
    "\n",
    "2. **Precision** measures the correctness of positive predictions:\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "$$\n",
    "\n",
    "3. **Recall** measures the ability to capture all positive instances:\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Statistics(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Generates statistical data: Accuracy, Precision and Recall.\n",
    "\n",
    "    Parameters:\n",
    "        y_test (DataFrame): Contains the true class labels for the data points in the test set.\n",
    "        y_pred (DataFrame): Contains the predicted class labels for the data points in the test set, generated by the trained classifier.\n",
    "    \n",
    "    Returns:\n",
    "        tuple (float, float, float): A tuple containing accuracy, precision, and recall scores.\n",
    "    \"\"\"\n",
    "    # Import libraries\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "\n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. ROC (Receiver Operating Characteristic) Curve and AUC (Area Under the ROC Curve)\n",
    "***ROC (Receiver Operating Characteristic) Curve*** is a graphical representation used to evaluate the performance of a binary classification model. It illustrates the trade-off between the true positive rate (TPR) and the false positive rate (FPR) at various threshold settings. Also known as sensitivity or recall, **True Positive Rate (TPR)** is the ratio of correctly predicted positive observations to the actual positives. **False Positive Rate (FPR)** is the ratio of incorrectly predicted positive observations to the actual negatives. The ROC curve is generated by varying the threshold for the classification model to determine what is considered a positive or negative prediction.\n",
    "$$\n",
    "\\text{FPR or Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{FPR} = \\frac{\\text{False Positives}}{\\text{False Positives} + \\text{True Negatives}}\n",
    "$$\n",
    "\n",
    "Interpreting the ROC Curve:\n",
    "- The curve plots TPR (y-axis) against FPR (x-axis). \n",
    "- A model with good predictive power will have a ROC curve that hugs the top left corner of the plot. This indicates high TPR and low FPR. \n",
    "- A random guess model would produce a diagonal line (45-degree line) from the bottom left to the top right, representing a TPR equal to FPR.\n",
    "\n",
    "    \n",
    "***AUC (Area Under the ROC Curve)*** is a single scalar value that summarizes the performance of the ROC curve, reflecting the model's ability to distinguish between positive and negative classes. \n",
    "\n",
    "Interpreting the AUC Area:\n",
    "- 1.0: Perfect model. The model perfectly distinguishes between positive and negative classes.\n",
    "- 0.8 to 0.9: Considered excellent. Models with AUC in this range have a high capability to differentiate between classes.\n",
    "- 0.7 to 0.8: Considered acceptable. These models generally perform adequately in distinguishing between classes.\n",
    "- 0.5 to 0.7: Considered reasonable. The closer to 0.7, the better the model's performance in distinguishing between classes.\n",
    "- 0.5: No discrimination. The model performs no better than random guessing.\n",
    "- < 0.5: Indicates a model that performs worse than random guessing, which typically indicates issues with the model or data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ROC_AUC(y_test, y_pred_proba_list, model):\n",
    "    \"\"\"\n",
    "    Plot ROC curve and calculate AUC for multiple probability predictions.\n",
    "\n",
    "    Parameters:\n",
    "        y_test (np.ndarray): True labels.\n",
    "        y_pred_proba_list (List[List[np.ndarray]]): List of lists of predicted probabilities from different models.\n",
    "        model (str): String to describe the algorithm being used.\n",
    "        \n",
    "    Returns:\n",
    "        None.           \n",
    "    \"\"\"\n",
    "    # Import libraries\n",
    "    from sklearn.metrics import roc_curve, roc_auc_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Create a figure with a size of 7x7 inches\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    \n",
    "    # Features composed by each subset\n",
    "    features = ['Low Level', 'High Level']\n",
    "    \n",
    "    # Define the colormap for each model type\n",
    "    color_maps = [plt.get_cmap('Reds'), plt.get_cmap('Greens'), plt.get_cmap('Blues')]\n",
    "    model_names = ['XGB', 'MLPK', 'MLPS']\n",
    "\n",
    "    # Iterate through each model type (XGB, MLPK, MLPS)\n",
    "    for j, model_preds in enumerate(y_pred_proba_list):\n",
    "        color_map = color_maps[j]\n",
    "        colors = [color_map((i+1) / len(model_preds)) for i in range(len(model_preds))]\n",
    "        \n",
    "        # Plot ROC for each set of predictions\n",
    "        for i, y_pred_proba in enumerate(model_preds):\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            plt.plot(fpr, tpr, color=colors[i], lw=2, \n",
    "                     label=f\"ROC curve (AUC = {roc_auc:.3f}) - {features[i]} {model_names[j]}\")\n",
    "\n",
    "    # Plot the diagonal line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    \n",
    "    # Plot the curves\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f\"Receiver Operating Characteristic (ROC) with {model}\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Plotting the results report\n",
    "The ResultsPlot function generates bar plots for model performance metrics (Accuracy, Precision, and Recall) from a provided report, allowing easy comparison of different models and feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ResultsPlot(report):\n",
    "    \"\"\"Plot the results from a report.\n",
    "\n",
    "    Parameters:\n",
    "        report (dict): Dictionary with Models and Metrics (Accuracy, Precision, and Recall).\n",
    "        \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Import libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.cm as cm\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Define the metrics and their corresponding subplot positions\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall']\n",
    "\n",
    "    # Flatten the list of model names\n",
    "    models_flat = [model for sublist in report['model'] for model in sublist]\n",
    "\n",
    "    # Create colormaps for different model types\n",
    "    color_maps = [cm.Reds, cm.Greens, cm.Blues]\n",
    "\n",
    "    # Loop through the metrics and plot them\n",
    "    for i, metric in enumerate(metrics):\n",
    "        # Plot the metric for each model\n",
    "        for j, model_type in enumerate(report['model']):\n",
    "            color_map = color_maps[j]\n",
    "            # Determine color based on model index within each model type\n",
    "            for k, model_name in enumerate(model_type):\n",
    "                color = color_map((k + 1) / len(model_type))\n",
    "                x_pos = k + j * len(report['model'][0])\n",
    "                axs[i].bar(x_pos, report[metric][j][k], color=color, label=model_name if i == 0 else \"\")\n",
    "            \n",
    "                # Add values on top of each bar\n",
    "                axs[i].text(x_pos, report[metric][j][k], '{:.2f}'.format(report[metric][j][k]), ha='center', va='bottom')\n",
    "        \n",
    "        axs[i].set_title(metric, fontsize=16)\n",
    "        axs[i].set_xlabel('Features_Model', fontsize=14)\n",
    "        axs[i].set_ylabel(metric, fontsize=14)\n",
    "        axs[i].set_xticks(range(len(models_flat)))\n",
    "        axs[i].set_xticklabels(models_flat, rotation=45, ha='right')\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Extreme Gradient Boosting\n",
    "\n",
    "Extreme Gradient Boosting (XGBoost) is a powerful boosting algorithm known for its efficiency and versatility:\n",
    "\n",
    "1. **Boosting Technique**: Sequentially combines weak learners to create a strong learner, improving predictive accuracy iteratively.\n",
    "   \n",
    "3. **Gradient Boosting**: Optimizes model parameters using gradient descent, reducing prediction errors by fitting new models to residuals.\n",
    "4. **Regularization**: Prevents overfitting and enhances generalization with techniques like tree pruning and feature subsampling.\n",
    "5. **Efficiency**: Utilizes parallel and distributed computing for scalability, handling large datasets efficiently.\n",
    "6. **Cross-validation and Early Stopping**: Supports hyperparameter tuning and prevents overfitting with cross-validation and early stopping techniques.\n",
    "\n",
    "We can also manage the behavior of this method using manually hyperparameters, such as:\n",
    "\n",
    "1. **n_estimators**: This hyperparameter controls the number of trees in the ensemble, influencing the model's overall complexity and performance.\n",
    "   \n",
    "3. **learning_rate**: By adjusting this hyperparameter, we determine the step size of each tree's contribution during the boosting process, affecting the rate of convergence and the model's generalization capability.\n",
    "4. **max_depth**: This hyperparameter specifies the maximum depth of each tree in the ensemble, influencing the complexity of individual trees and, consequently, the model's ability to capture complex patterns in the data.\n",
    "5. **early_stopping_rounds**: This hyperparameter allows the model to automatically halt training when the performance metric on a validation dataset stops improving for a specified number of consecutive boosting rounds (early_stopping_rounds). This helps prevent overfitting, and ensures efficient use of computational resources. It enhances model robustness by stopping training at the point where further iterations no longer improve validation performance, thereby optimizing model training and performance.\n",
    "\n",
    "In the following function, **tracking time** (start_time, end_time, run_time) ensures transparency regarding the computational cost of training. The predicted probabilities (y_pred_proba) are crucial for tasks such as computing Receiver Operating Characteristic (ROC) curves and calculating the Area Under the Curve (AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtremeGradientBoosting(X_train, y_train, X_val, y_val, X_test, y_test, COLS, \\\n",
    "                            n_estimators, learning_rate, max_depth, \\\n",
    "                            early_stopping_rounds=70, eval_metric=\"merror\"):\n",
    "    \"\"\"\n",
    "    Train and predict with Extreme Gradient Boosting.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (Union[pd.Series, np.ndarray]): Training labels.\n",
    "        X_val (pd.DataFrame): Validation features.\n",
    "        y_val (Union[pd.Series, np.ndarray]): Validation labels.\n",
    "        X_test (pd.DataFrame): Test features.\n",
    "        y_test (Union[pd.Series, np.ndarray]): Test labels.\n",
    "        COLS (List[str]): Set of features/columns with which the model is trained and predicts.\n",
    "        n_estimators (int): Controls the number of trees in the ensemble.\n",
    "        learning_rate (float): Determines the step size of each tree's contribution.\n",
    "        max_depth (int): Specifies the maximum depth of each tree, influencing its complexity.\n",
    "        early_stopping_rounds (int, optional): Number of rounds of boosting to perform before stopping. Default is 70.\n",
    "        eval_metric (str, optional): Evaluation metric used to monitor performance on the validation set. Default is \"merror\".\n",
    "        \n",
    "    Returns:\n",
    "        Tuple: A tuple containing the following elements:\n",
    "            - statistics (Tuple[float, float, float]): Evaluation metrics for the test set (accuracy, precision, recall).\n",
    "            - evals_result (Dict[str, Dict[str, List[float]]]): Evaluation results of the model on the training and validation sets.\n",
    "            - y_pred_proba (np.ndarray): Predicted probabilities for the test set.\n",
    "            - xgb_clf (xgb.XGBClassifier): Trained XGBoost model.\n",
    "    \"\"\"\n",
    "    # Import libraries\n",
    "    import time\n",
    "    import xgboost as xgb\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Print start time\n",
    "    print(\"Start time:\", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize the Gradient Boosting Classifier with specified hyperparameters\n",
    "    xgb_clf = xgb.XGBClassifier(\n",
    "        objective='multi:softmax', \n",
    "        num_class=2, \n",
    "        n_estimators=n_estimators, \n",
    "        learning_rate=learning_rate, \n",
    "        max_depth=max_depth,\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        eval_metric=eval_metric,\n",
    "    )\n",
    "    \n",
    "    # Train the classifier on the training data with evaluation set\n",
    "    xgb_clf.fit(\n",
    "        X_train[COLS], \n",
    "        y_train, \n",
    "        eval_set=[(X_train[COLS], y_train), (X_val[COLS], y_val)], \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Get evaluation results\n",
    "    evals_result = xgb_clf.evals_result()\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = xgb_clf.predict(X_test[COLS])\n",
    "    \n",
    "    # Predict on the test data for ROC/AUC\n",
    "    y_pred_proba = xgb_clf.predict_proba(X_test[COLS])[:, 1]\n",
    "    \n",
    "    # Record end time\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate and print time taken\n",
    "    run_time = end_time - start_time\n",
    "    print(f\"\\nTime taken for running Extreme Gradient Boosting: {run_time/60:.2f} minutes.\\n\")\n",
    "    \n",
    "    # Return metrics, evaluation results, predictions and model\n",
    "    return Statistics(y_test, y_pred), evals_result, y_pred_proba, xgb_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is useful for monitoring the training process of an XGBoost model by visually inspecting how both training and validation errors evolve over the course of training. It helps in diagnosing issues such as overfitting or underfitting, and assessing the overall training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def XGBTrainValidationLossPlot(evals_result, columns):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss for a Extreme Gradient Boosting model.\n",
    "        \n",
    "    Parameters:\n",
    "        evals_result (dict): Provides a dictionary containing the evaluation results of the model during training.\n",
    "        columns (str): String to describe the columns being analyzed.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Plot learning curves\n",
    "    epochs = len(evals_result['validation_0']['merror'])\n",
    "    x_axis = range(0, epochs)\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.plot(x_axis, evals_result['validation_0']['merror'], label='Train')\n",
    "    plt.plot(x_axis, evals_result['validation_1']['merror'], label='Validation')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Boosting Rounds')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f\"XGBoost Loss with {columns}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1. Low Level features\n",
    "We start by training and predicting with the Low-Level features using the Extreme Gradient Boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extreme Gradient Boosting Classifier with Low Level features\n",
    "(acc_XGB_ll, prec_XGB_ll, rec_XGB_ll), evals_result_XGB_ll, y_pred_proba_XGB_ll, clf_XGB_ll = ExtremeGradientBoosting(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, LowLevelCols, \\\n",
    "    50, 0.1, 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "XGBTrainValidationLossPlot(evals_result_XGB_ll, 'Low-Level features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation loss curve closely follows the train loss curve. The process is halted at boosting round 50 because the values start to diverge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2. High Level features\n",
    "We finally train and predict using the Extreme Gradient Boosting model with the High-Level features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extreme Gradient Boosting Classifier with High Level features\n",
    "(acc_XGB_hl, prec_XGB_hl, rec_XGB_hl), evals_result_XGB_hl, y_pred_proba_XGB_hl, clf_XGB_hl = ExtremeGradientBoosting(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, HighLevelCols, \\\n",
    "    50, 0.1, 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "XGBTrainValidationLossPlot(evals_result_XGB_hl, 'High-Level features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation loss curve closely follows the train loss curve. The process is halted at boosting round 50 because the values start to diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROC_AUC(y_test, [[y_pred_proba_XGB_ll, y_pred_proba_XGB_hl],], 'Extreme Gradient Boosting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret the AUC values for this model as follows:\n",
    "\n",
    "1. **Low-Level Extreme Gradient Boosting AUC = 0.675**:\n",
    "\n",
    "This value falls between 0.5 and 0.7, which is considered reasonable. It indicates that the model's performance in distinguishing between classes is not very strong but better than random guessing. It suggests that while the Low-Level features model has some ability to differentiate between positive and negative classes, there is significant room for improvement.\n",
    "\n",
    "2. **High-Level Extreme Gradient Boosting AUC = 0.786**:\n",
    "\n",
    "This value falls between 0.7 and 0.8, which is considered acceptable. It indicates that the model performs adequately in distinguishing between classes. The High-Level features model shows a better capability to differentiate between classes compared to the low-level feature model, reflecting a noticeable improvement in performance.\n",
    "\n",
    "Based on the AUC values and their interpretation, the conclusion for this model's performance is: **This model, although powerful, has however difficulties discovering patterns in the Low-Level features**, as indicated by the AUC of 0.675, which is considered reasonable but not strong. On the other hand, the model performs well with High-Level features, as demonstrated by the AUC of 0.786, which falls into the acceptable range, indicating a good capability to distinguish between Background and Signal classes. This suggests that the model benefits significantly from High-Level features and can effectively leverage them to improve its predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1.4. Results\n",
    "Now we plot the statistical results (Accuracy, Precion and Recall) for the Extreme Gradient Boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the results\n",
    "report = {'model': [['Low_XGB', 'High_XGB']],\n",
    "          'Accuracy': [[acc_XGB_ll, acc_XGB_hl]],\n",
    "          'Precision': [[prec_XGB_ll, prec_XGB_hl]],\n",
    "          'Recall': [[rec_XGB_ll, rec_XGB_hl]]\n",
    "         }\n",
    "\n",
    "ResultsPlot(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Multilayer Perceptrons\n",
    "\n",
    "Multilayer Perceptrons (MLP) are a type of artificial neural network composed of multiple layers of interconnected neurons, structured as follows:\n",
    "\n",
    "1. **Neurons (Units)**: Neurons receive input signals, process them, and produce output signals. They are organized into layers: input, hidden, and output.\n",
    "   \n",
    "2. **Layers**:\n",
    "    - *Input Layer*: Receives raw input data, with each neuron representing a feature.\n",
    "    - *Hidden Layers*: Perform nonlinear transformations of input data to learn abstract representations.\n",
    "    - *Output Layer*: Produces final predictions or outputs based on learned representations.\n",
    "      <p></p> \n",
    "3. **Connections and Weights**: Neurons in adjacent layers are connected by weighted connections, determining signal strength. Weights are adjusted during training to minimize prediction errors.\n",
    "4. **Activation Functions**: Introduce nonlinearity into the network, enabling complex mappings between inputs and outputs. Common functions include sigmoid, tanh, and ReLU.\n",
    "5. **Feedforward Propagation**: MLP use feedforward propagation to compute predictions. Input data passes through the network, undergoing weighted sums and activation functions at each layer.\n",
    "6. **Training**: Typically conducted using supervised learning, where the network learns to map input data to target outputs. Training involves forward propagation, backpropagation for gradient computation, and weight optimization.\n",
    "7. **Versatility and Performance**: MLP are versatile models capable of learning complex patterns in data. They may require careful tuning and regularization to prevent overfitting, especially with high-dimensional or noisy data.\n",
    "\n",
    "Below are the differences between the two different MLP implementation architectures used: in this project:\n",
    "1. **Keras/TensorFlow**. Keras is a high-level neural network API that runs on top of TensorFlow, a powerful framework for building, training, and deploying Deep Learning models. It offers extensive flexibility in defining neural network architectures, including the ability to create complex models with various types of layers (e.g., Dense, Convolutional, Recurrent) and custom activation functions. Keras also supports advanced configurations like dropout regularization, batch normalization, and custom training loops. It is well-suited for tasks requiring deep customization and control over the neural network architecture.\n",
    "   \n",
    "3. **Scikit-learn**. Scikit-learn, on the other hand, is a general-purpose machine learning library in Python focused on simplicity and ease of use. It includes an MLP implementation primarily designed for traditional machine learning tasks like classification and regression. The MLP in Scikit-learn is defined using parameters such as hidden_layer_sizes, activation, and solver, offering a simpler approach to constructing neural networks compared to Keras. Scikit-learn integrates well with other scientific Python libraries and provides straightforward methods for model training, evaluation, and deployment.\n",
    "\n",
    "In the following two MLP models, the importance given to Early Stopping and Loss Monitoring can be explained as follows:\n",
    "\n",
    "1. **Early Stopping**: This mechanism helps prevent overfitting by stopping training when validation performance no longer improves. By doing so, it ensures that the model can generalize well to new data, as evidenced by a consistent reduction in validation loss.\n",
    "\n",
    "2. **Train/Validation Loss**: Monitoring these losses provides crucial insights into the model's performance. Significant discrepancies between training and validation losses may indicate overfitting, where the model memorizes specific details of the training data instead of learning general patterns.\n",
    "\n",
    "These practices are essential for developing models that not only fit the training data well but also generalize effectively to unseen data, thereby enhancing their utility and reliability in practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1. Multilayer Perceptrons with Keras/TensorFlow\n",
    "\n",
    "To fine-tune the number of neurons in dense layers, dropout rate, and batch size in your MLPKeras function, we follow these guidelines:\n",
    "1. **Number of Neurons** in Dense layers: Adjusts the value to scale neurons in each dense layer. It's typical to start with a baseline number of neurons per layer, such as, for larger datasets, 128, or 256.\n",
    "\n",
    "2. **Dropout Rate** in Dropout layers: It refers to a regularization technique used to prevent overfitting during the training process, and works by randomly deactivating (or \"dropping out\") a fraction of neurons in a layer during each training iteration. \n",
    "\n",
    "3. **Batch Size** in Fit function: Determines the number of samples processed before updating model weights, influencing training speed and generalization. If we have a large dataset and ample GPU memory, larger batch sizes might be suitable for faster training; for smaller datasets or when training on CPUs, smaller batch sizes might be necessary to fit into memory and ensure the model learns well.\n",
    "\n",
    "In the following function, **tracking time** (start_time, end_time, run_time) ensures transparency regarding the computational cost of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPKeras(X_train, y_train, X_val, y_val, X_test, y_test, COLS, \\\n",
    "             neurons_base, dropout_rate_coef, batch_size):\n",
    "    \"\"\"\n",
    "    Train and predict with Multilayer Perceptrons with Keras.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (Union[pd.Series, np.ndarray]): Training labels.\n",
    "        X_val (pd.DataFrame): Validation features.\n",
    "        y_val (Union[pd.Series, np.ndarray]): Validation labels.\n",
    "        X_test (pd.DataFrame): Test features.\n",
    "        y_test (Union[pd.Series, np.ndarray]): Test labels.\n",
    "        COLS (List[str]): Set of features/columns with which the model is trained and predicts.\n",
    "        neurons_base (int): Base value for neurons.\n",
    "        dropout_rate_coef (float): Dropout Rate Coefficient.\n",
    "        batch_size (int): Batch Size.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple: A tuple containing the following elements:\n",
    "            - statistics (Tuple[float, float, float]): Evaluation metrics for the test set (accuracy, precision, recall).\n",
    "            - history (tf.keras.callbacks.History): Training history of the model.\n",
    "            - y_pred_proba (np.ndarray): Predicted probabilities for the test set.\n",
    "            - mlpk_clf (tensorflow.keras.Model): Trained Keras MLP model.\n",
    "    \"\"\"\n",
    "    # Import necessary libraries\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import Model\n",
    "    from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "    # Print start time\n",
    "    print(\"Start time:\", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize MLPKeras model using the Functional API\n",
    "    input_shape = len(COLS)\n",
    "    inputs = Input(shape=(input_shape,))\n",
    "    x = Dense(neurons_base * 1, activation='relu')(inputs)\n",
    "    x = Dropout(dropout_rate_coef * 0.1)(x)\n",
    "    x = Dense(neurons_base * 2, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate_coef * 0.1)(x)\n",
    "    x = Dense(neurons_base * 4, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate_coef * 0.1)(x)\n",
    "    x = Dense(neurons_base * 8, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate_coef * 0.1)(x)            \n",
    "    x = Dense(neurons_base * 4, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate_coef * 0.1)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    mlpk_clf = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Print summary of the model\n",
    "    mlpk_clf.summary()\n",
    "    \n",
    "    # Compile the model\n",
    "    print(\"Compiling the Model...\")\n",
    "    mlpk_clf.compile(\n",
    "        optimizer='adam', \n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=5, \n",
    "        verbose=1, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    print(\"Training the Model...\")\n",
    "    history = mlpk_clf.fit(\n",
    "        X_train[COLS], \n",
    "        y_train, \n",
    "        epochs=100,\n",
    "        batch_size=batch_size, \n",
    "        validation_data=(X_val[COLS], y_val),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Predict using the trained model\n",
    "    print(\"Predicting on the Testset...\")\n",
    "    y_pred_proba = mlpk_clf.predict(X_test[COLS])\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Record end time\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate and print time taken\n",
    "    run_time = end_time - start_time\n",
    "    print(f\"Time taken for running Multilayer Perceptrons with Keras: {run_time/60:.2f} minutes.\")\n",
    "\n",
    "    # Return metrics, history, predicted labels and model\n",
    "    return Statistics(y_test, y_pred), history, y_pred_proba, mlpk_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following UMAP function visualizes the UMAP projection of the second-to-last dense layer outputs from a trained Keras MLP model. Applying UMAP (Uniform Manifold Approximation and Projection) to the penultimate layer of Deep Learning models is particularly useful for several reasons:\n",
    "\n",
    "1. **Capturing Abstract Representations**: The penultimate layer of a deep neural network often contains abstract and semantic representations of the input data. As data is processed through the layers of the network, it is transformed into higher-level representations that encapsulate complex and task-relevant features.\n",
    "\n",
    "2. **Dimensionality Reduction**: Representations in the penultimate layer typically have a relatively high dimensionality, which can make visualization and direct interpretation of learned patterns challenging. Reducing the dimensionality of these representations with UMAP preserves important data relationships while projecting them into a lower-dimensional space, facilitating visualization and analysis.\n",
    "\n",
    "3. **Interpretability**: Applying UMAP to the penultimate layer allows exploration of how the model organizes and separates data in a simpler and more interpretable representation space. This can reveal natural clusters in the data, separation patterns between classes, or important features learned by the model.\n",
    "\n",
    "4. **Efficient Visualization**: Visualizing outputs from the penultimate layer provides insights into how the model processes and understands data. UMAP enables the creation of visualizations that help data scientists and researchers better understand the model's behavior and interpret its decisions.\n",
    "\n",
    "5. **Model Comparison**: By applying UMAP to outputs from the penultimate layer of different models, it is possible to compare how different neural network architectures or training parameters result in different data representations. This can be useful for model tuning or identifying specific aspects of the problem that each model captures.\n",
    "\n",
    "In summary, applying UMAP to the penultimate layer of deep learning is a powerful technique for exploring and interpreting the internal representations learned by the model, providing a clearer and more intuitive view of the learning process and data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MLPKerasUMAP(mlpk_clf, TrainSet, y_train, columns):\n",
    "    \"\"\"\n",
    "    Visualizes the UMAP projection of the second-to-last dense layer outputs from a given Keras MLP model.\n",
    "\n",
    "    Parameters:\n",
    "        mlpk_clf (tensorflow.keras.Model): Trained Keras MLP model.\n",
    "        TrainSet (numpy.ndarray): Training data used to obtain the intermediate layer outputs.\n",
    "        y_train (numpy.ndarray): Labels corresponding to the training data.\n",
    "        columns (str): String to describe the columns being analyzed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Import libraries\n",
    "    import time\n",
    "    from tensorflow.keras import Model\n",
    "    import umap.umap_ as umap\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.lines import Line2D\n",
    "\n",
    "    # Print start time\n",
    "    print(\"Start time:\", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "\n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create a model to output the second-to-last dense layer\n",
    "    intermediate_layer_model = Model(inputs=mlpk_clf.input, outputs=mlpk_clf.layers[-3].output)\n",
    "    \n",
    "    # Get the intermediate layer outputs for the training data\n",
    "    print(\"Predicting on the Second-to-last Dense Layer...\")\n",
    "    intermediate_output = intermediate_layer_model.predict(TrainSet)\n",
    "    \n",
    "    # Use UMAP to reduce dimensionality\n",
    "    reducer = umap.UMAP()\n",
    "    print(\"Embedding...\")\n",
    "    embedding = reducer.fit_transform(intermediate_output)\n",
    "    \n",
    "    # Plot the UMAP embedding\n",
    "    print(\"Plotting...\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cmap = plt.cm.viridis\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], c=y_train, cmap=cmap, s=5, alpha=0.5)\n",
    "    \n",
    "    # Create custom legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap(cmap.N), markersize=10, label='Signal'),\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap(0), markersize=10, label='Background')    ]\n",
    "    plt.legend(handles=legend_elements, title='Classes', loc='lower right')\n",
    "    \n",
    "    plt.title(f\"UMAP projection of the second-to-last layer outputs for {columns} with Keras MLP\")\n",
    "    plt.xlabel('UMAP Dimension 1')\n",
    "    plt.ylabel('UMAP Dimension 2')\n",
    "    plt.show()\n",
    "    \n",
    "    # Record end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate and print time taken\n",
    "    run_time = end_time - start_time\n",
    "    print(f\"Time taken for Embedding and Plotting: {run_time/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Train/Validation Loss Plot function is used to visualize the training and validation loss curves of a Keras MLP model across epochs. It takes a history object as input, which contains recorded metrics from the model training process, such as loss values for both training and validation data. By plotting these loss curves, the function provides insights into how well the model is learning over time. The x-axis represents the number of epochs, while the y-axis shows the corresponding loss values. The plot helps in understanding:\n",
    "\n",
    "1. **Training Progress**: It shows whether the model is improving its performance on the training data as epochs progress.\n",
    "   \n",
    "3. **Generalization Capability**: By comparing the training and validation loss curves, it indicates how well the model generalizes to unseen validation data. Large discrepancies between these curves might suggest overfitting or underfitting.\n",
    "\n",
    "This visualization, being crucial for monitoring training progress and assessing model convergence and potential issues like overfitting or underfitting, helps make informed decisions regarding model adjustments or training strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MLPKerasTrainValidationLossPlot(history, columns):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss for a MLPKeras model.\n",
    "\n",
    "    Parameters:\n",
    "        history (History class): History object returned by the fit method of a Keras model.\n",
    "        columns (str): String to describe the columns being analyzed.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Import libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Get the number of epochs\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    \n",
    "    # Plot the training and validation loss\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.plot(epochs, history.history['loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    # Set x-ticks at regular intervals\n",
    "    plt.xticks(np.arange(0, max(epochs), 5))\n",
    "    \n",
    "    plt.title(f\"Training and Validation Loss for {columns} with Keras MLP\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.1.1. Low Level features\n",
    "We start by training and predicting with the Low-Level features using Keras Multilayer Perceptrons model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Multilayer Perceptrons with Keras with Low Level features\n",
    "(acc_MLPK_ll, prec_MLPK_ll, rec_MLPK_ll), history_MLPK_ll, y_pred_proba_MLPK_ll, clf_MLPK_ll = MLPKeras(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, LowLevelCols, \\\n",
    "    128, 1, 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MLPKerasUMAP(clf_MLPK_ll, X_train[LowLevelCols], y_train, 'Low-Level features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the UMAP projection for the Low-Level features, the following observations can be made:\n",
    "\n",
    "1. **Two Clusters**: There are two main clusters:\n",
    "   - One at the top labeled as \"Background\".\n",
    "   - One at the bottom labeled as \"Signal\".\n",
    "<p></p>\n",
    "\n",
    "2. **Mixed Main Area**: The central area of the UMAP projection is occupied by a mix of data points from both \"Signal\" and \"Background\" labels. This suggests that there is no clear separation between the two clusters in this region, indicating significant overlap or similarity in features.\n",
    "\n",
    "3. **Outliers**: There are two sets of outliers that are distinct from the main area and clusters. These outliers are situated away from both the central mixed area and the main clusters, indicating they have unique characteristics.\n",
    "\n",
    "We can draw the following conclusions about this clustering:\n",
    "\n",
    "1. **Distinct Clusters with Mixed Region**: The data points form two primary clusters, \"Signal\" and \"Background,\" but the main area of the UMAP projection shows a mix of both labels. This implies that while there are distinct groups, there is a significant portion of the data where the features of \"Signal\" and \"Background\" overlap.\n",
    "\n",
    "2. **Overlap Indicates Ambiguity**: The mixing of \"Signal\" and \"Background\" labels in the central area suggests that the boundary between these two clusters is not well-defined. This could indicate that the features used for clustering do not completely differentiate between the two labels, leading to ambiguity in classification.\n",
    "\n",
    "3. **Outliers**: The presence of outliers, which do not fit into the main clusters or the mixed central area, indicates there are data points with unique features. These outliers may represent noise, rare cases, or potentially new subgroups that are not captured by the main clusters.\n",
    "\n",
    "4. **Implications for Further Analysis**: The overlap in the main area suggests that the model requires a larger dataset. It could be also beneficial to analyze the features of the data points in the mixed region to understand the source of overlap better and to explore if additional features or a different clustering approach might improve separation. The outliers should be investigated to determine their nature and relevance to the overall data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MLPKerasTrainValidationLossPlot(history_MLPK_ll, 'Low-Level features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation loss curve closely follows the train loss curve. The process is halted at epoch 30 because the values start to diverge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.1.2. High Level features\n",
    "We finally train and predict using Keras Multilayer Perceptrons model with the High-Level features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Perceptrons with Keras with High Level features\n",
    "(acc_MLPK_hl, prec_MLPK_hl, rec_MLPK_hl), history_MLPK_hl, y_pred_proba_MLPK_hl, clf_MLPK_hl = MLPKeras(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, HighLevelCols, \\\n",
    "    128, 1, 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MLPKerasUMAP(clf_MLPK_hl, X_train[HighLevelCols], y_train, 'High-Level features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the UMAP projection for the High-Level features, the following observations can be made:\n",
    "\n",
    "1. **Larger Clusters**: Compared to the clusters obtained with the Low-Level features, the two main clusters have become slightly larger:\n",
    "   - One on the top left labeled as \"Background.\"\n",
    "   - One on the top right labeled as \"Signal.\"\n",
    "\n",
    "2. **Mixed Main Area**: The central area, extending from the bottom left to the bottom right, is occupied by a mix of data points from both \"Signal\" and \"Background\" labels. This suggests significant overlap or similarity in features in this region.\n",
    "\n",
    "3. **Reduced Outliers**: The majority of the outliers observed in the Low-Level features projection have disappeared. The data points are now more concentrated within the two main clusters and the mixed central area.\n",
    "\n",
    "We can draw the following conclusions about this clustering:\n",
    "\n",
    "1. **Expanded Clusters**: The \"Background\" and \"Signal\" clusters are now slightly larger, indicating that the High-Level features have helped to group more data points into the main clusters, reducing ambiguity.\n",
    "\n",
    "2. **Decreased Ambiguity in Overlap**: The central mixed area, which extends from the bottom left to the bottom right, is still present, but with a significantly lower number of outliers, suggesting that the High-Level features provide a clearer differentiation between the two labels. The overlap remains, indicating some ambiguity, but it has decreased compared to the Low-Level features.\n",
    "\n",
    "3. **Outliers Reduction**: The reduction in the number of outliers implies that the High-Level features have captured more relevant information, bringing previously outlying points into the main clusters or mixed area. This suggests a more robust clustering model with better-defined groups.\n",
    "\n",
    "4. **Implications for Further Analysis**: The persistent central mixing region, extending from the bottom left to the bottom right, indicates that there is still some overlap between the features of the two labels; analyzing the data points in this area could provide insights into the nature of this overlap. The decrease in outliers suggests that the model's representation of the data has improved; further refinement could focus on enhancing feature extraction to further reduce the central overlap and increase cluster distinctiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPKerasTrainValidationLossPlot(history_MLPK_hl, 'High-Level features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation loss curve closely follows the train loss curve. The process is halted at epoch 22 because the values start to diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROC_AUC(y_test, [[y_pred_proba_XGB_ll, y_pred_proba_XGB_hl], \\\n",
    "                 [y_pred_proba_MLPK_ll, y_pred_proba_MLPK_hl],], 'XGBoosting and Keras MLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret the AUC values for the model MLP with Keras as follows:\n",
    "\n",
    "1. **Low-Level Multilayer Perceptrons (MLP) with Keras AUC = 0.725:**\n",
    "\n",
    "This value falls between 0.7 and 0.8, which is considered acceptable. It indicates that the model performs adequately in distinguishing between classes. The Low-Level features model shows a better capability to differentiate between positive and negative classes compared to random guessing, reflecting a noticeable improvement in performance over the Low-Level XGB model.\n",
    "\n",
    "2. **High-Level Multilayer Perceptrons (MLP) with Keras AUC = 0.792:**\n",
    "\n",
    "This value also falls between 0.7 and 0.8, which is considered acceptable. It indicates that the model performs well in distinguishing between classes. The High-Level features model shows a slightly better capability to differentiate between classes compared to the High-Level XGB model, indicating a good capability to distinguish between Background and Signal classes.\n",
    "\n",
    "Based on the AUC values and their interpretation, the conclusion for XGB and MLPK models' performance is:\n",
    "\n",
    "1. **Low-Level Features:** The Low-Level MLP model has an AUC of 0.725, which falls into the acceptable range. This indicates that the MLP with Keras has a reasonable capability to differentiate between classes, showing better performance than the Low-Level XGB model with an AUC of 0.675. It suggests that the MLP model is better at discovering patterns in the Low-Level features compared to the XGB model, though there is still room for improvement, eventually with a larger dataset.\n",
    "\n",
    "2. **High-Level Features:** The High-Level MLP model has an AUC of 0.792, which is in the acceptable range and slightly better than the High-Level XGB model's AUC of 0.786. This indicates that the MLP with Keras performs well with High-Level features, showing a good capability to distinguish between Background and Signal classes. The performance is comparable to the XGB model, suggesting that both models benefit significantly from High-Level features and can effectively leverage them to improve predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the results\n",
    "report = {'model': [['Low_XGB', 'High_XGB'],['Low_MLPK', 'High_MLPK']],\n",
    "          'Accuracy': [[acc_XGB_ll, acc_XGB_hl],[acc_MLPK_ll, acc_MLPK_hl]],\n",
    "          'Precision': [[prec_XGB_ll, prec_XGB_hl],[prec_MLPK_ll, prec_MLPK_hl]],\n",
    "          'Recall': [[rec_XGB_ll, rec_XGB_hl],[rec_MLPK_ll, rec_MLPK_hl]]\n",
    "         }\n",
    "\n",
    "ResultsPlot(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2. Multilayer Perceptrons with Scikit-learn\n",
    "\n",
    "Scikit-learn, a widely-used machine learning library, does not have built-in functionalities specifically developed for loss monitoring in its MLPClassifier for Multilayer Perceptrons. This capability is typically found in Deep Learning frameworks like TensorFlow or PyTorch, which offer more extensive tools for neural network training and optimization. So, in the following MLPScikit function, the manual implementation of early stopping with loss monitoring serves several critical purposes to ensure effective model training and performance evaluation:\n",
    "\n",
    "1. **Early Stopping**: Early stopping is crucial for preventing overfitting. It terminates the training process when the validation accuracy stops improving, thereby preventing the model from learning noise or specific details of the training data that do not generalize well. In the function, early stopping is manually implemented by monitoring the validation accuracy (val_accuracy). If there is no improvement in validation accuracy for a specified number of epochs (patience=5), training is halted early to avoid overfitting.\n",
    "\n",
    "2. **Train/Validation Loss Monitoring**: Monitoring these losses provides insights into how well the model is learning and generalizing from the training data to unseen validation data. The Training Loss is calculated using the logarithmic loss (log_loss) between the predicted probabilities (y_train_pred_proba) and actual training labels (y_train); this metric helps assess how well the model fits the training data. Similarly, the Validation Loss is calculated using log_loss between predicted probabilities (y_val_pred_proba) and actual validation labels (y_val). It indicates how well the model generalizes to unseen validation data.\n",
    "\n",
    "In the following function, the following hyperparameters are finely tuned:\n",
    "\n",
    "1. **Activation** Specifies the activation function used in the MLP (e.g., 'relu', 'sigmoid'). Activation functions introduce non-linearity to the model, enabling it to capture complex patterns in the data effectively.\n",
    "\n",
    "2. **Solver** Determines the optimization algorithm for weight optimization ('adam', 'lbfgs', 'sgd'). Each solver has its advantages depending on the dataset size, complexity, and available computational resources.\n",
    "\n",
    "3. **Epochs**: Refers to the number of complete passes through the training dataset. Each epoch involves forward and backward propagation, updating model parameters to minimize training loss and enhance performance iteratively.\n",
    "\n",
    "Also, **tracking time** (start_time1, start_time2, end_time1, end_time2, run_time1, run_time2) ensures transparency regarding the computational cost of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MLPScikit(X_train, y_train, X_val, y_val, X_test, y_test, COLS, \\\n",
    "              activation, solver, epochs):\n",
    "    \"\"\"\n",
    "    Train and predict with Scikit-learn MLP Model.\n",
    "\n",
    "    Parameters:\n",
    "      X_train (pd.DataFrame): Training features.\n",
    "      y_train (Union[pd.Series, np.ndarray]): Training labels.\n",
    "      X_val (pd.DataFrame): Validation features.\n",
    "      y_val (Union[pd.Series, np.ndarray]): Validation labels.\n",
    "      X_test (pd.DataFrame): Test features.\n",
    "      y_test (Union[pd.Series, np.ndarray]): Test labels.\n",
    "      COLS (List[str]): Set of features/columns with which the model is trained and predicts.\n",
    "      activation (str): Activation function.\n",
    "      solver (str): Solver algorithm.\n",
    "      epochs (int): Number of epochs for training.\n",
    "\n",
    "    Returns:\n",
    "      Tuple: A tuple containing the following elements:\n",
    "          - statistics (Tuple[float, float, float]): Evaluation metrics for the test set (accuracy, precision, recall).\n",
    "          - history (List[List[float]]): Training history of the model (training loss, validation loss).\n",
    "          - y_pred_proba (np.ndarray): Predicted probabilities for the test set.\n",
    "          - best_mlps_model (MLPClassifier): Best trained Scikit Multilayer Perceptron model.\n",
    "    \"\"\"\n",
    "    # Import necessary libraries\n",
    "    import time\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import accuracy_score, log_loss\n",
    "    import numpy as np\n",
    "\n",
    "    # Print start time\n",
    "    print(\"Start time:\", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "\n",
    "    # Record start time\n",
    "    start_time1 = time.time()\n",
    "\n",
    "    # Initialize the MLPClassifier\n",
    "    mlps_model = MLPClassifier(\n",
    "        hidden_layer_sizes=(128, 256, 512, 1024, 512, 1),\n",
    "        alpha=0.001, # L2 regularization\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        random_state=42,\n",
    "        early_stopping=False  # Turn off built-in early stopping\n",
    "    )\n",
    "    \n",
    "    # Parameters of the model\n",
    "    print(\"Parameters of MLP Scikit model in use:\")\n",
    "    params = mlps_model.get_params()\n",
    "    for key, value in params.items():\n",
    "        print(f\"{key:<20} {value}\")\n",
    "    print()\n",
    "\n",
    "    # Initialize variables for early stopping\n",
    "    best_val_accuracy = 0\n",
    "    patience = 5\n",
    "    best_mlps_model = None\n",
    "    best_epoch = None\n",
    "    epochs_without_improvement = 0\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        # Record start time\n",
    "        start_time2 = time.time()\n",
    "        print(f\"Epoch {epoch+1:03}:\", end=\" \")\n",
    "\n",
    "        # Fit the model to the training data\n",
    "        print(f\"Fitting...\", end=\" \")\n",
    "        mlps_model.partial_fit(X_train[COLS], y_train, classes=np.unique(y_train))\n",
    "        \n",
    "        # Track training loss\n",
    "        print(f\"Train_loss:\", end=\" \")\n",
    "        y_train_pred_proba = mlps_model.predict_proba(X_train[COLS])\n",
    "        train_loss = log_loss(y_train, y_train_pred_proba)\n",
    "        print(f\"{train_loss:.4f}\", end=\" \")\n",
    "        training_loss.append(train_loss)\n",
    "\n",
    "        # Calculate validation accuracy \n",
    "        print(f\"Val_accuracy:\", end=\" \")\n",
    "        y_val_pred = mlps_model.predict(X_val[COLS])\n",
    "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        accuracy_improvement = val_accuracy - best_val_accuracy\n",
    "        print(f\"{val_accuracy:.4f} Acc_improv.: {accuracy_improvement:+.5f}\", end=\" \")\n",
    "        \n",
    "        # Calculate validation loss \n",
    "        print(f\"Val_loss:\", end=\" \")\n",
    "        y_val_pred_proba = mlps_model.predict_proba(X_val[COLS])\n",
    "        val_loss = log_loss(y_val, y_val_pred_proba)\n",
    "        print(f\"{val_loss:.4f}\", end=\" \")\n",
    "        validation_loss.append(val_loss)\n",
    "\n",
    "        # Check for epochs without improvement\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_mlps_model = mlps_model\n",
    "            best_epoch = epoch\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        # Print epochs without improvement\n",
    "        print(f\"Epochs w/o improv.: {epochs_without_improvement}.\", end=\" \")\n",
    "        \n",
    "        # Record end time\n",
    "        end_time2 = time.time()\n",
    "        \n",
    "        # Calculate and print time taken\n",
    "        run_time2 = end_time2 - start_time2\n",
    "        print(f\"{run_time2/60:.2f} min.\")\n",
    "\n",
    "        # Check for break in early stopping\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "    # Create history\n",
    "    history = [training_loss, validation_loss]\n",
    "\n",
    "    # Use the best model for predictions\n",
    "    print(f\"\\nPredicting with the model of epoch {best_epoch+1}...\")\n",
    "    y_pred = best_mlps_model.predict(X_test[COLS])\n",
    "    \n",
    "    # Predict on the test data for ROC/AUC\n",
    "    y_pred_proba = best_mlps_model.predict_proba(X_test[COLS])[:, 1]\n",
    "\n",
    "    # Record end time\n",
    "    end_time1 = time.time()\n",
    "\n",
    "    # Calculate and print time taken\n",
    "    run_time1 = end_time1 - start_time1\n",
    "    print(f\"Time taken for running Scikit-learn MLP Model: {run_time1/60:.2f} minutes.\")\n",
    "\n",
    "    # Return metrics and predicted labels\n",
    "    return Statistics(y_test, y_pred), history, y_pred_proba, best_mlps_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function provides a visual UMAP representation of how well the Scikit-learn MLP model separates classes in a lower-dimensional space, based on its learned representations from the second-to-last dense layer outputs. We can adjust sample_fraction to control the proportion of data used for embedding, depending on the computational resources available and the desired level of detail in the visualization.\n",
    "\n",
    "In the following function, tracking time (start_time, end_time, run_time) ensures transparency regarding the computational cost of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MLPScikitUMAP(mlps_model, TrainSet, y_train, columns, sample_fraction=0.1):\n",
    "    \"\"\"\n",
    "    Visualizes the UMAP projection of the second-to-last dense layer outputs\n",
    "    from a given Scikit-learn MLP model, using a sample of the training data.\n",
    "\n",
    "    Parameters:\n",
    "        mlps_model (sklearn.neural_network.MLPClassifier): Trained Scikit-learn MLP model.\n",
    "        TrainSet (numpy.ndarray): Training data used to obtain the second-to-last layer outputs.\n",
    "        y_train (numpy.ndarray): Labels corresponding to the training data.\n",
    "        sample_fraction (float): Fraction of the data to use for embedding (default is 0.1, i.e., 10%).\n",
    "        columns (str): String to describe the columns being analyzed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Import necessary libraries\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import umap\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.lines import Line2D\n",
    "\n",
    "    # Print start time\n",
    "    print(\"Start time:\", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "\n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define the function to extract the intermediate output\n",
    "    def get_intermediate_output(model, data):\n",
    "        # Perform a forward pass up to the second-to-last layer\n",
    "        layer_activations = data\n",
    "        for i in range(len(model.coefs_) - 1):  # Exclude the last layer\n",
    "            layer_activations = np.dot(layer_activations, model.coefs_[i]) + model.intercepts_[i]\n",
    "            if model.activation == 'relu':\n",
    "                layer_activations = np.maximum(0, layer_activations)\n",
    "            elif model.activation == 'logistic':\n",
    "                layer_activations = 1 / (1 + np.exp(-layer_activations))\n",
    "            elif model.activation == 'tanh':\n",
    "                layer_activations = np.tanh(layer_activations)\n",
    "            elif model.activation == 'identity':\n",
    "                pass\n",
    "        return layer_activations\n",
    "    \n",
    "    # Get the intermediate layer outputs for the training data\n",
    "    print(\"Predicting on the Second-to-last Layer...\")\n",
    "    intermediate_output = get_intermediate_output(mlps_model, TrainSet)\n",
    "    \n",
    "    # Reset the index of y_train to make it compatible with the random indices\n",
    "    y_train_reset = y_train.reset_index(drop=True)\n",
    "    \n",
    "    # Sample the data\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    sample_size = int(len(intermediate_output) * sample_fraction)\n",
    "    sample_indices = np.random.choice(len(intermediate_output), sample_size, replace=False)\n",
    "    sample_output = intermediate_output[sample_indices]\n",
    "    sample_labels = y_train_reset[sample_indices]\n",
    "\n",
    "    # Use UMAP to reduce dimensionality\n",
    "    reducer = umap.UMAP(n_neighbors=15, metric='euclidean', n_epochs=200, init='spectral', n_jobs=-1, verbose=True)\n",
    "    print(\"Embedding...\")\n",
    "    embedding = reducer.fit_transform(sample_output)\n",
    "    \n",
    "    # Plot the UMAP embedding\n",
    "    print(\"Plotting...\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cmap = plt.cm.viridis\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], c=sample_labels, cmap=cmap, s=5, alpha=0.5)\n",
    "    \n",
    "    # Create custom legend\n",
    "    legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap(0), markersize=10, label='Background'),\n",
    "                       Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap(cmap.N), markersize=10, label='Signal')\n",
    "                      ]\n",
    "    plt.legend(handles=legend_elements, title='Classes')\n",
    "    \n",
    "    # Add title, x-axis label, y-axis label, and display the plot\n",
    "    plt.title(f\"UMAP projection of the second-to-last layer outputs for {columns} with Scikit-learn MLP\")\n",
    "    plt.xlabel('UMAP Dimension 1')\n",
    "    plt.ylabel('UMAP Dimension 2')\n",
    "    plt.show()\n",
    "    \n",
    "    # Record end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate and print time taken\n",
    "    run_time = end_time - start_time\n",
    "    print(f\"Time taken for Embedding and Plotting: {run_time/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is essential for visualizing the Training and Validation Loss trends over epochs, in the Scikit-learn MLP model, aiding in assessing the model's training progress and identifying potential issues such as overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MLPScikitTrainValidationLossPlot(history, columns):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss for a Scikit-learn MLP model.\n",
    "\n",
    "    Parameters:\n",
    "        history (List[List[float]]): List containing training and validation loss (history from MLP function).\n",
    "        columns (str): String to describe the columns being analyzed.\n",
    "        \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Import libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Plot the training and validation loss\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.plot(history[0], label='Training Loss')\n",
    "    plt.plot(history[1], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f\"Training and Validation Loss for {columns} with Scikit-learn MLP\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.2.1. Low Level features\n",
    "We start by training and predicting with the Low-Level features using Scikit-learn Multilayer Perceptrons model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Perceptrons with Scikit-learn with Low Level features\n",
    "(acc_MLPS_ll, prec_MLPS_ll, rec_MLPS_ll), history_MLPS_ll, y_pred_proba_MLPS_ll, clf_MLPS_ll = MLPScikit(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, LowLevelCols, \\\n",
    "    'tanh', 'sgd', 50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MLPScikitUMAP(clf_MLPS_ll, X_train[LowLevelCols], y_train, 'Low-Level features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We examine this UMAP visualization derived from an MLP model implemented using the scikit-learn library, with a sample size of 10% of the total dataset with Low-Level features:\n",
    "\n",
    "1. **Little clusters on bottom (Background on left, Signal on right)**: The UMAP visualization reveals small clusters located at the bottom of the plot. These clusters represent groups of similar data points, with a distinct distribution: Background data points on the left and Signal data points on the right. This separation suggests a potential pattern or relationship within the data.\n",
    "\n",
    "2. **Majority mix of both**: The overall UMAP plot shows a blend of both Background and Signal data points across most of the visualization. This intermingling indicates that the data is not completely separable into distinct groups, but rather a mix of both types of data points is present, suggesting overlap or integration.\n",
    "\n",
    "3. **Comparison with Keras**: It is noted that these clusters are slightly smaller compared to those observed when using a Multilayer Perceptron with Keras, indicating a bit worse performance than the Keras implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPScikitTrainValidationLossPlot(history_MLPS_ll, 'Low-Level features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation loss curve closely follows the train loss curve. The process is halted at epoch 50 because the values start to diverge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.2.2. High Level features\n",
    "We finally train and predict using Scikit-learn Multilayer Perceptrons model with the High-Level features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Perceptrons with Scikit-learn with High Level features\n",
    "(acc_MLPS_hl, prec_MLPS_hl, rec_MLPS_hl), history_MLPS_hl, y_pred_proba_MLPS_hl, clf_MLPS_hl = MLPScikit(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, HighLevelCols, \\\n",
    "    'tanh', 'sgd', 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MLPScikitUMAP(clf_MLPS_hl, X_train[HighLevelCols], y_train, 'High-Level features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now examine this UMAP visualization derived from an MLP model implemented using the scikit-learn library, with a sample size of 10% of the total dataset with High-Level features:\n",
    "\n",
    "1. **Comparison with Low-Level features**: In comparison to Low-Level features, the UMAP plot with High-Level features shows slightly larger clusters at the bottom. This suggests that the use of High-Level features has resulted in a different clustering pattern, possibly indicating a more distinct separation between Background and Signal data points. However, both feature types exhibit a majority mix of both Background and Signal data points throughout the visualization.\n",
    "\n",
    "2. **Comparison with Keras**: When compared with a Multilayer Perceptron implemented using Keras, the clusters observed in the UMAP plot with High-Level features are slightly smaller. This indicates that the scikit-learn MLP with High-Level features may have lower performance in capturing the underlying structure of the data compared to the Keras implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPScikitTrainValidationLossPlot(history_MLPS_hl, 'High-Level features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation loss curve closely tracks the training loss curve. The process is halted at epoch 100 because the improvements that are made are very minimal and do not justify the processing cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROC_AUC(y_test, [[y_pred_proba_XGB_ll, y_pred_proba_XGB_hl], \\\n",
    "                 [y_pred_proba_MLPK_ll, y_pred_proba_MLPK_hl], \\\n",
    "                 [y_pred_proba_MLPS_ll, y_pred_proba_MLPS_hl],], 'XGB, MLPK and MLPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous ROC/AUC plot, the following observations can be made:\n",
    "\n",
    "1. **Low-Level Features: AUC = 0.706**\n",
    "The AUC value of 0.706 for the Scikit-learn MLP model with Low-Level features suggests moderate performance in distinguishing between classes. Falling within the acceptable range (typically 0.7 to 0.8), it indicates decent discriminatory power but also suggests potential for improvement.\n",
    "\n",
    "2. **High-Level Features: AUC = 0.791**\n",
    "The AUC value of 0.791 for the Scikit-learn MLP model with High-Level features indicates better performance compared to the Low-Level model. This higher AUC value suggests that the model performs well in distinguishing between classes and demonstrates a stronger discriminatory ability.\n",
    "\n",
    "The slight difference in performance between the MLP with Scikit-learn and the MLP with Keras, with Keras being slightly better, could be attributed to several factors:\n",
    "\n",
    "- **Model Complexity:** Keras allows for more flexibility and customization in model architecture and training parameters compared to Scikit-learn. This flexibility may enable Keras models to capture complex patterns in data more effectively.\n",
    "\n",
    "- **Backend Implementation:** Keras utilizes TensorFlow or another backend framework optimized for neural network computations. This backend optimization can lead to improved performance compared to Scikit-learn's more generalized approach.\n",
    "\n",
    "- **Feature Handling:** Keras models may handle features differently or more optimally than Sscikit-learn MLPs. This difference in feature handling can influence how well the models discriminate between classes, potentially giving Keras models a slight performance advantage. However, Scikit-learn remains a robust choice for MLP implementations due to its ease of use and integration into broader Machine Learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the results\n",
    "report = {'model': [['Low_XGB', 'High_XGB'],['Low_MLPK', 'High_MLPK'],['Low_MLPS', 'High_MLPS']],\n",
    "          'Accuracy': [[acc_XGB_ll, acc_XGB_hl],[acc_MLPK_ll, acc_MLPK_hl],[acc_MLPS_ll, acc_MLPS_hl]],\n",
    "          'Precision': [[prec_XGB_ll, prec_XGB_hl],[prec_MLPK_ll, prec_MLPK_hl],[prec_MLPS_ll, prec_MLPS_hl]],\n",
    "          'Recall': [[rec_XGB_ll, rec_XGB_hl],[rec_MLPK_ll, rec_MLPK_hl],[rec_MLPS_ll, rec_MLPS_hl]]\n",
    "         }\n",
    "\n",
    "ResultsPlot(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Based on the analysis and interpretations of AUC values for Extreme Gradient Boosting (XGB), Multilayer Perceptrons (MLP) with Keras, and MLP with Scikit-learn, let's delve deeper into how Deep Learning methods, especially MLPs, excel over shallow models like XGB, particularly when dealing with raw or Low-Level features in high-energy physics data:\n",
    "\n",
    "### Deep Learning vs. Shallow Models in Handling Low-Level Features\n",
    "\n",
    "1. **Feature Engineering and Model Performance:**\n",
    "   - **XGB Performance (Low-Level):** XGB achieves an AUC of 0.675 with low-level features, indicating moderate performance. This suggests that XGB, while capable, struggles to extract complex patterns directly from raw or minimally processed data without additional feature engineering.\n",
    "   - **MLP with Scikit-learn Performance (Low-Level):** Similarly, MLP with Scikit-learn achieves an AUC of 0.706 with Low-Level features, slightly better than XGB. This indicates that Scikit-learn MLP, with its neural network architecture, can leverage some nonlinear relationships but still requires feature engineering to optimize performance.\n",
    "\n",
    "2. **Deep Learning Advantages (Low-Level Features):**\n",
    "   - **MLP with Keras Performance (Low-Level):** In contrast, MLP with Keras achieves an AUC of 0.725 with Low-Level features, demonstrating improved capability compared to XGB and Scikit-learn MLP, even with a relatively small dataset of 1 million observations. This improvement suggests that Keras, with its TensorFlow backend, allows for more flexible and deeper model architectures that can learn intricate patterns directly from raw data.\n",
    "\n",
    "3. **Complex Pattern Learning:**\n",
    "   - **Capability of Deep Learning:** Deep learning, particularly MLPs implemented with Keras, excels in learning complex nonlinear functions directly from data without relying heavily on manually constructed features. This is crucial in high-energy physics where discovering subtle patterns in raw detector data is paramount.\n",
    "   - **Advantages Over Shallow Models:** MLPs with Keras can automatically discover and leverage powerful combinations of features, especially Low-Level, to enhance classification accuracy. This ability reduces the dependence on domain expertise for feature engineering, thereby accelerating model development and improving adaptability to varying data characteristics.\n",
    "\n",
    "4. **Model Flexibility and Optimization:**\n",
    "   - **Backend and Architecture:** Keras provides a robust framework optimized for neural network computations, which enhances the efficiency of model training and inference. This backend optimization is particularly advantageous in handling large-scale, high-dimensional data typical in particle physics experiments.\n",
    "\n",
    "### Conclusion and Implications\n",
    "\n",
    "Deep Learning methods, especially MLPs implemented with Keras/TensorFlow, outperform shallow models like XGB when dealing with Low-Level features in high-energy physics data classification tasks. The ability of MLPs to autonomously learn complex patterns from raw data underscores their superiority in capturing nuanced distinctions between Signal and Background classes.\n",
    "\n",
    "For future advancements in high-energy physics data analysis:\n",
    "- **Automated Feature Discovery:** Deep Learning's capability to automatically extract discriminative features from raw data can lead to more accurate and robust classification models.\n",
    "- **Reduced Manual Intervention:** The reduced need for manual feature engineering allows physicists to focus more on scientific insights and less on preprocessing data, potentially accelerating discoveries of exotic particles and phenomena.\n",
    "\n",
    "In summary, leveraging Deep Learning, particularly MLPs with Keras, represents a significant step forward in addressing the complexities of high-energy physics data analysis, enhancing both accuracy and efficiency in classification tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "name": "Lab_2_ML_pipeline",
  "notebookId": 296256117212061
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
